{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "folder_path = \"./data\"\n",
    "\n",
    "with open('stopwords.txt', 'r') as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace('_', '')\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    stemmed_tokens = [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# sort files by number\n",
    "def natural_key(filename):\n",
    "    return [int(text) if text.isdigit() else text for text in re.split(r'(\\d+)', filename)]\n",
    "\n",
    "def read_documents(folder_path):\n",
    "    documents = {}\n",
    "\n",
    "    for filename in sorted(os.listdir(folder_path), key=natural_key):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_id = filename[:-4]\n",
    "            \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                content = preprocess(content)\n",
    "                documents[file_id] = content\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate df\n",
    "def calculate_df(documents):\n",
    "    doc_count = defaultdict(int)\n",
    "    \n",
    "    # Count number of documents containing each term\n",
    "    for tokens in documents.values():\n",
    "        unique_terms = set(tokens)\n",
    "        for term in unique_terms:\n",
    "            doc_count[term] += 1\n",
    "    return doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary to dictionary.txt\n",
    "def save_dictionary_to_file(doc_count, output_filename=\"dictionary.txt\"):\n",
    "    dictionary = [(term, df) for term, df in doc_count.items()]\n",
    "    \n",
    "    dictionary.sort(key=lambda x: x[0])\n",
    "    \n",
    "    indexed_dictionary = [(index + 1, term, df) for index, (term, df) in enumerate(dictionary)]\n",
    "    \n",
    "    term_to_index = {term: t_index for t_index, term, df in indexed_dictionary}\n",
    "    \n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"t_index\\tterm\\tdf\\n\")\n",
    "        for t_index, term, df in indexed_dictionary:\n",
    "            file.write(f\"{t_index}\\t{term}\\t{df}\\n\")\n",
    "    \n",
    "    print(f\"Dictionary saved to {output_filename}\")\n",
    "    \n",
    "    return term_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tf-idf\n",
    "def calculate_tfidf(documents):\n",
    "    doc_count = calculate_df(documents)\n",
    "    total_documents = len(documents)\n",
    "    \n",
    "    tfidf_vectors = {}\n",
    "    \n",
    "    for doc_id, tokens in documents.items():\n",
    "        tf = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "            \n",
    "        # Normalize TF\n",
    "        total_terms = len(tokens)\n",
    "        for term in tf:\n",
    "            tf[term] /= total_terms\n",
    "        \n",
    "        tfidf = {}\n",
    "        for term, term_tf in tf.items():\n",
    "            idf = math.log10(total_documents / (1 + doc_count[term])) # log10\n",
    "            tfidf[term] = term_tf * idf\n",
    "        \n",
    "        # Convert to unit vector\n",
    "        norm = math.sqrt(sum(value ** 2 for value in tfidf.values()))\n",
    "        tfidf_unit_vector = {term: value / norm for term, value in tfidf.items()} if norm > 0 else tfidf\n",
    "        \n",
    "        tfidf_vectors[doc_id] = tfidf_unit_vector\n",
    "    \n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tf-idf unit vector to output/\n",
    "def save_tfidf_to_files(tfidf_vectors, term_to_index, output_folder=\"./output\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, (doc_id, vector) in enumerate(tfidf_vectors.items(), start=1):\n",
    "        output_filename = os.path.join(output_folder, f\"{index}.txt\")\n",
    "        \n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"{len(vector)}\\n\")\n",
    "            file.write(\"t_index\\ttf-idf\\n\")\n",
    "            \n",
    "            sorted_tfidf = [(term_to_index[term], tfidf) for term, tfidf in vector.items() if term in term_to_index]\n",
    "            sorted_tfidf.sort(key=lambda x: x[0])\n",
    "            \n",
    "            for t_index, tfidf_value in sorted_tfidf:\n",
    "                file.write(f\"{t_index}\\t{tfidf_value}\\n\")\n",
    "        \n",
    "        # print(f\"TF-IDF vector for Document {doc_id} saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "\n",
    "def read_tfidf_file(filename):\n",
    "    tfidf_vector = {}\n",
    "    \n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()[2:]\n",
    "        for line in lines:\n",
    "            t_index, tfidf_value = line.strip().split('\\t')\n",
    "            tfidf_vector[int(t_index)] = float(tfidf_value)\n",
    "    \n",
    "    return tfidf_vector\n",
    "\n",
    "def cosine(Docx, Docy):\n",
    "    # Read TF-IDF vectors for the two documents\n",
    "    tfidf_vector_x = read_tfidf_file(f\"./output/{Docx}.txt\")\n",
    "    tfidf_vector_y = read_tfidf_file(f\"./output/{Docy}.txt\")\n",
    "    \n",
    "    # Find the common t_indices\n",
    "    common_indices = set(tfidf_vector_x.keys()) & set(tfidf_vector_y.keys())\n",
    "    \n",
    "    dot_product = sum(tfidf_vector_x[i] * tfidf_vector_y[i] for i in common_indices)\n",
    "    \n",
    "    norm_x = math.sqrt(sum(value ** 2 for value in tfidf_vector_x.values()))\n",
    "    norm_y = math.sqrt(sum(value ** 2 for value in tfidf_vector_y.values()))\n",
    "    \n",
    "    if norm_x == 0 or norm_y == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    cosine_similarity = dot_product / (norm_x * norm_y)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to dictionary.txt\n",
      "Cosine Similarity between Document 1 and Document 2 is: 0.1881508600743812\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "documents = read_documents(folder_path)\n",
    "doc_count = calculate_df(documents)\n",
    "\n",
    "term_to_index = save_dictionary_to_file(doc_count)\n",
    "\n",
    "tfidf_vectors = calculate_tfidf(documents)\n",
    "\n",
    "save_tfidf_to_files(tfidf_vectors, term_to_index)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine(1, 2)\n",
    "print(f\"Cosine Similarity between Document 1 and Document 2 is: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
