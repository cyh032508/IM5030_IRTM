{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = text.replace('_', '')\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_tokens = [porter.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def load_training_data(file_path):\n",
    "    training_data = defaultdict(list)\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tokens = list(map(int, line.split()))\n",
    "            class_label = tokens[0]\n",
    "            training_data[class_label].extend(tokens[1:])\n",
    "    return training_data\n",
    "\n",
    "def read_files(file_indices, folder):\n",
    "    data = []\n",
    "    labels = []\n",
    "    file_ids = []\n",
    "    for class_label, indices in file_indices.items():\n",
    "        for idx in indices:\n",
    "            file_path = os.path.join(folder, f\"{idx}.txt\")\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                processed_tokens = preprocess(content)\n",
    "                data.append(processed_tokens)\n",
    "                labels.append(class_label)\n",
    "                file_ids.append(idx)\n",
    "    return data, labels, file_ids\n",
    "\n",
    "def compute_chi2(data, labels, vocab):\n",
    "    word_class_count = defaultdict(lambda: defaultdict(int))\n",
    "    class_count = defaultdict(int)\n",
    "    total_word_count = sum(len(tokens) for tokens in data)\n",
    "\n",
    "    for tokens, label in zip(data, labels):\n",
    "        class_count[label] += 1\n",
    "        for word in tokens:\n",
    "            if word in vocab:\n",
    "                word_class_count[word][label] += 1\n",
    "\n",
    "    chi2_stats = {}\n",
    "    for word in vocab:\n",
    "        observed = word_class_count[word]\n",
    "        row_sum = sum(observed.values())\n",
    "        chi2 = 0\n",
    "        for label in class_count:\n",
    "            expected = (row_sum * class_count[label]) / total_word_count\n",
    "            observed_freq = observed.get(label, 0)\n",
    "            chi2 += (observed_freq - expected) ** 2 / expected if expected > 0 else 0\n",
    "        chi2_stats[word] = chi2\n",
    "    return chi2_stats\n",
    "\n",
    "def select_features_with_chi2(data, labels, top_k=1000):\n",
    "    vocab = set(word for tokens in data for word in tokens)\n",
    "    chi2_stats = compute_chi2(data, labels, vocab)\n",
    "    sorted_features = sorted(chi2_stats.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    selected_features = [word for word, _ in sorted_features]\n",
    "    return selected_features\n",
    "\n",
    "def train_multinomial_nb(data, labels, vocab):\n",
    "    class_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "    class_doc_counts = defaultdict(int)\n",
    "    total_docs = len(labels)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    for tokens, label in zip(data, labels):\n",
    "        class_doc_counts[label] += 1\n",
    "        for word in tokens:\n",
    "            if word in vocab:\n",
    "                class_word_counts[label][word] += 1\n",
    "\n",
    "    class_cond_probs = {}\n",
    "    class_probs = {}\n",
    "    for label, doc_count in class_doc_counts.items():\n",
    "        total_word_count = sum(class_word_counts[label].values()) + vocab_size\n",
    "        class_probs[label] = log(doc_count / total_docs)\n",
    "        class_cond_probs[label] = {\n",
    "            word: log((class_word_counts[label].get(word, 0) + 1) / total_word_count)\n",
    "            for word in vocab\n",
    "        }\n",
    "    return class_probs, class_cond_probs\n",
    "\n",
    "def predict(tokens, vocab, class_probs, class_cond_probs):\n",
    "    scores = {}\n",
    "    for label in class_probs:\n",
    "        score = class_probs[label]\n",
    "        for word in tokens:\n",
    "            if word in vocab:\n",
    "                score += class_cond_probs[label].get(word, 0)\n",
    "        scores[label] = score\n",
    "    return max(scores, key=scores.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been saved to 'predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "training_file = \"./training.txt\"\n",
    "data_folder = \"./data\"\n",
    "top_k = 500\n",
    "\n",
    "training_data = load_training_data(training_file)\n",
    "\n",
    "train_data, train_labels, _ = read_files(training_data, data_folder)\n",
    "\n",
    "selected_features = select_features_with_chi2(train_data, train_labels, top_k)\n",
    "\n",
    "train_data_selected = [\n",
    "    [word for word in tokens if word in selected_features] \n",
    "    for tokens in train_data\n",
    "]\n",
    "\n",
    "class_probs, class_cond_probs = train_multinomial_nb(train_data_selected, train_labels, selected_features)\n",
    "\n",
    "test_indices = set(range(1, 1096)) - {idx for indices in training_data.values() for idx in indices}\n",
    "test_data, test_labels, test_file_ids = read_files({None: list(test_indices)}, data_folder)\n",
    "\n",
    "predictions = []\n",
    "for tokens, file_id in zip(test_data, test_file_ids):\n",
    "    selected_tokens = [word for word in tokens if word in selected_features]\n",
    "    predicted_label = predict(selected_tokens, selected_features, class_probs, class_cond_probs)\n",
    "    predictions.append((file_id, predicted_label))\n",
    "\n",
    "df = pd.DataFrame(predictions, columns=[\"Id\", \"Value\"])\n",
    "df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "print(\"Predictions have been saved to 'predictions.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
