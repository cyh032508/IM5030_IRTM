{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理 k=8 的聚類...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 143\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m正在處理 k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 的聚類...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m hac_clustering(distance_matrix, doc_ids, k)\n\u001b[1;32m    144\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters_k_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     save_results(clusters, file_ids, output_file)\n",
      "Cell \u001b[0;32mIn[3], line 96\u001b[0m, in \u001b[0;36mhac_clustering\u001b[0;34m(distance_matrix, doc_ids, k)\u001b[0m\n\u001b[1;32m     93\u001b[0m closest_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (doc_id_1, doc_id_2), dist \u001b[38;5;129;01min\u001b[39;00m distance_matrix\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;241m<\u001b[39m min_dist:\n\u001b[1;32m     97\u001b[0m         min_dist \u001b[38;5;241m=\u001b[39m dist\n\u001b[1;32m     98\u001b[0m         closest_pair \u001b[38;5;241m=\u001b[39m (doc_id_1, doc_id_2)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = text.replace('_', '')\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_tokens = [porter.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def read_files(folder):\n",
    "    data = []\n",
    "    file_ids = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_id = filename.split('.')[0]  # 檔案名的數字部分\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                processed_tokens = preprocess(content)\n",
    "                data.append(processed_tokens)\n",
    "                file_ids.append(file_id)\n",
    "    return data, file_ids\n",
    "\n",
    "def calculate_df(documents):\n",
    "    doc_count = defaultdict(int)\n",
    "    for tokens in documents:\n",
    "        unique_terms = set(tokens)\n",
    "        for term in unique_terms:\n",
    "            doc_count[term] += 1\n",
    "    return doc_count\n",
    "\n",
    "def calculate_tfidf(documents):\n",
    "    doc_count = calculate_df(documents)\n",
    "    total_documents = len(documents)\n",
    "    \n",
    "    tfidf_vectors = {}\n",
    "    for doc_id, tokens in documents.items():\n",
    "        tf = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "        \n",
    "        total_terms = len(tokens)\n",
    "        for term in tf:\n",
    "            tf[term] /= total_terms\n",
    "        \n",
    "        tfidf = {}\n",
    "        for term, term_tf in tf.items():\n",
    "            idf = math.log10(total_documents / (1 + doc_count[term]))\n",
    "            tfidf[term] = term_tf * idf\n",
    "        \n",
    "        norm = math.sqrt(sum(value ** 2 for value in tfidf.values()))\n",
    "        tfidf_unit_vector = {term: value / norm for term, value in tfidf.items()} if norm > 0 else tfidf\n",
    "        \n",
    "        tfidf_vectors[doc_id] = tfidf_unit_vector\n",
    "    \n",
    "    return tfidf_vectors\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(vec1.get(term, 0) * vec2.get(term, 0) for term in set(vec1) | set(vec2))\n",
    "    magnitude_1 = math.sqrt(sum(value ** 2 for value in vec1.values()))\n",
    "    magnitude_2 = math.sqrt(sum(value ** 2 for value in vec2.values()))\n",
    "    if magnitude_1 == 0 or magnitude_2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude_1 * magnitude_2)\n",
    "\n",
    "def calculate_distance_matrix(tfidf_vectors):\n",
    "    doc_ids = list(tfidf_vectors.keys())\n",
    "    distance_matrix = {}\n",
    "    for i, doc_id_1 in enumerate(doc_ids):\n",
    "        for j, doc_id_2 in enumerate(doc_ids):\n",
    "            if i < j:\n",
    "                sim = cosine_similarity(tfidf_vectors[doc_id_1], tfidf_vectors[doc_id_2])\n",
    "                distance_matrix[(doc_id_1, doc_id_2)] = 1 - sim\n",
    "                distance_matrix[(doc_id_2, doc_id_1)] = 1 - sim\n",
    "    return distance_matrix, doc_ids\n",
    "\n",
    "def hac_clustering(distance_matrix, doc_ids, k):\n",
    "    clusters = {doc_id: [doc_id] for doc_id in doc_ids}\n",
    "    while len(clusters) > k:\n",
    "        min_dist = float('inf')\n",
    "        closest_pair = None\n",
    "\n",
    "        for (doc_id_1, doc_id_2), dist in distance_matrix.items():\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_pair = (doc_id_1, doc_id_2)\n",
    "        \n",
    "        doc_id_1, doc_id_2 = closest_pair\n",
    "        cluster_1 = clusters.pop(doc_id_1, None)\n",
    "        cluster_2 = clusters.pop(doc_id_2, None)\n",
    "        if cluster_1 is None or cluster_2 is None:\n",
    "            continue\n",
    "        \n",
    "        new_cluster_id = f\"{doc_id_1}_{doc_id_2}\"\n",
    "        clusters[new_cluster_id] = cluster_1 + cluster_2\n",
    "        \n",
    "        keys_to_delete = [(doc_id_1, key) for key in clusters.keys()] + \\\n",
    "                         [(doc_id_2, key) for key in clusters.keys()]\n",
    "        for key in keys_to_delete:\n",
    "            if key in distance_matrix:\n",
    "                del distance_matrix[key]\n",
    "        \n",
    "        for other_id in clusters.keys():\n",
    "            dist_1 = min(distance_matrix.get((doc_id_1, other_id), float('inf')), \n",
    "                         distance_matrix.get((other_id, doc_id_1), float('inf')))\n",
    "            dist_2 = min(distance_matrix.get((doc_id_2, other_id), float('inf')), \n",
    "                         distance_matrix.get((other_id, doc_id_2), float('inf')))\n",
    "            distance_matrix[(new_cluster_id, other_id)] = min(dist_1, dist_2)\n",
    "            distance_matrix[(other_id, new_cluster_id)] = min(dist_1, dist_2)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def save_results(clusters, file_ids, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for cluster_id, cluster in clusters.items():\n",
    "            for doc_id in cluster:\n",
    "                f.write(f\"{doc_id} {cluster_id}\\n\")\n",
    "    print(f\"結果已保存至 {output_file}\")\n",
    "\n",
    "folder = \"./data\"\n",
    "data, file_ids = read_files(folder)\n",
    "\n",
    "documents = {file_ids[i]: data[i] for i in range(len(data))}\n",
    "tfidf_vectors = calculate_tfidf(documents)\n",
    "\n",
    "distance_matrix, doc_ids = calculate_distance_matrix(tfidf_vectors)\n",
    "\n",
    "k_values = [8, 13, 20]\n",
    "for k in k_values:\n",
    "    print(f\"正在處理 k={k} 的聚類...\")\n",
    "    clusters = hac_clustering(distance_matrix, doc_ids, k)\n",
    "    output_file = f\"clusters_k_{k}.txt\"\n",
    "    save_results(clusters, file_ids, output_file)\n",
    "    print(f\"k={k} 的聚類結果已保存至 {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
